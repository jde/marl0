services:
  app:
    build:
      context: ./app
    depends_on:
      - kafka
      - postgres
      - memgraph
      - redis
      - minio
      - prometheus
      - grafana
    volumes:
      - ./app:/usr/src/app
    ports:
      - "4200:3000"
    environment:
      - NODE_ENV=development
    command: ["npm", "run", "dev"]

  product-api:
    build: ./services/product-api
    ports:
      - "42420:3000"
    environment:
      - DATABASE_URL=postgres://youruser:yourpass@postgres:5432/marl0
      - KAFKAJS_NO_PARTITIONER_WARNING=1
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    depends_on:
      postgres:
        condition: service_healthy
      bootstrap:
        condition: service_completed_successfully
    volumes:
      - ./services/product-api:/app
    command: sh -c "npx prisma migrate deploy && npm run dev"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 10s                
      timeout: 3s
      retries: 15

  view-elasticsearch:
    build:
      context: ./agents/view-elasticsearch
    container_name: view-elasticsearch
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BROKERS=kafka:9092
      - PRODUCT_API_URL=http://product-api:3000
      # optional experiment filter
      # - EXPERIMENT=my-experiment

  # ingestor:
  #   build:
  #     context: ./services/ingestor
  #   container_name: ingestor
  #   ports:
  #     - "3000:3000" # Prometheus metrics
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   networks:
  #     - default

  # entity-extractor:
  #   build:
  #     context: ./services/entity-extractor
  #   container_name: entity-extractor
  #   ports:
  #     - "4444:3000" # Prometheus metrics
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #     ingestor:
  #       condition: service_started
  #     local-llm:  
  #       condition: service_healthy
  #   networks:
  #     - default
  #   environment:
  #     - LOCAL_LLM_URL=${LOCAL_LLM_URL}
  #     - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL}

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/9092"]
      interval: 1s
      timeout: 5s
      retries: 60
    
  kafka-init:
    image: bitnami/kafka:latest
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: >
      bash -c "
        sleep 2 &&
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic sitemap.entries &&
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic story.cleaned &&
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic marl0.firehose &&
        kafka-topics.sh --create --if-not-exists --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic entity.candidates &&
        echo 'âœ… Kafka topics ready.'
      "

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  elasticsearch:
    image: opensearchproject/opensearch:2.11.1
    environment:
      - discovery.type=single-node
      - plugins.security.disabled=true
      - bootstrap.memory_lock=true
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200"]
      interval: 10s
      timeout: 5s
      retries: 10

# We may want to run llms natively 
  # local-llm:
  #   image: ollama/ollama
  #   ports:
  #     - "11434:11434" # Ollama API default port
  #   volumes:
  #     - ./docker_data/ollama:/models
  #   environment:
  #     - OLLAMA_MODELS=/models   
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "ollama list > /dev/null 2>&1"]
  #     interval: 2s
  #     timeout: 5s
  #     retries: 300

  postgres:
    image: postgres:latest
    environment:
      POSTGRES_USER: youruser
      POSTGRES_PASSWORD: yourpass
      POSTGRES_DB: yourdb
    ports:
      - "4242:5432"
    volumes:
      - ${POSTGRES_DATA_DIR}:/var/lib/postgresql/data
      - ./services/postgres/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U youruser"]
      interval: 1s
      timeout: 5s
      retries: 30
      
  lancedb:
    build:
      context: ./services/lancedb
      dockerfile: Dockerfile
    ports:
      - "4280:8080"
    volumes:
      - ${LANCEDB_DATA_DIR}:/data

  memgraph:
    image: memgraph/memgraph-platform:latest
    ports:
      - "4287:7687"
      - "4201:3000" # Memgraph Lab UI
    volumes:
      - ${MEMGRAPH_DATA_DIR}:/var/lib/memgraph

  redis:
    image: redis:latest
    ports:
      - "4279:6379"
    volumes:
      - ${REDIS_DATA_DIR}:/data

  minio:
    image: minio/minio:latest
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "4202:9000"
      - "4203:9001"
    command: server /data --console-address ":9001"
    volumes:
      - ${MINIO_DATA_DIR}:/data

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "4204:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    ports:
      - "4205:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ${GRAFANA_DATA_DIR}:/var/lib/grafana
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
    depends_on:
      - prometheus

  # curl:
  #   image: curlimages/curl:latest
  #   entrypoint: ["sleep", "infinity"]
  #   networks:
  #     - default

  # Pull models at build time
  # Keep up to date with all models we want to use locally
  # ollama-init:
  #   build:
  #     context: ./services/ollama-init
  #   volumes:
  #     - ${OLLAMA_DATA_DIR}:/root/.ollama
  #   environment:
  #     - OLLAMA_MODELS=/models

  bootstrap:
    build:
      context: ./services/bootstrap
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      POSTGRES_USER: youruser
      POSTGRES_PASSWORD: yourpass
      POSTGRES_DB: yourdb
      MARL0_POSTGRES_USER: youruser
      MARL0_POSTGRES_PASSWORD: yourpass
      MARL0_POSTGRES_DB: marl0
    restart: "no"

  #
  # Agents
  #

  sitemap-watcher:
    build:
      context: ./agents/functions/sitemap-watcher-go
      dockerfile: ./Dockerfile
    container_name: agent-sitemap-watcher
    depends_on:
      kafka-init:
        condition: service_completed_successfully
    volumes:
      - ${AGENT_SITEMAP_WATCHER_DATA_DIR}:/app/seen-urls.db
    environment:
      - KAFKA_BROKERS=kafka:9092

  story-writer:
    build:
      context: ./agents/story-writer
      dockerfile: ./Dockerfile
    container_name: agent-story-writer
    depends_on:
      kafka-init:
        condition: service_completed_successfully
    environment:
      - KAFKA_BROKERS=kafka:9092

  story-fetcher:
    build:
      context: ./agents/story-fetcher
      dockerfile: Dockerfile
    container_name: agent-story-fetcher
    depends_on:
      kafka-init:
        condition: service_completed_successfully
    environment:
      - KAFKA_BROKERS=kafka:9092
      - KAFKA_TOPIC=story.cleaned
      - AGENT_NAME=story-fetcher
      - AGENT_VERSION=v1
      - GROUP_INSTANCE=dev2
      - METRICS_PORT=9100
      - LOG_LEVEL=debug


